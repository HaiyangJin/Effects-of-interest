---
title             : 'Appendices for ""'
shorttitle         : "(Pre-)specifiying effects of interest"

author: 
  - name: "Haiyang Jin"
    affiliation: ""
    address: ""
    email: "haiyang.jin@outlook.com"
    corresponding: yes
    
affiliation:
  - id            : ""
    institution   : ""
  
authornote        : |
  This is the appendices for "".
keywords          : ""
wordcount         : "X"

figurelist        : false
tablelist         : false
footnotelist      : false
linenumbers       : false
mask              : false
draft             : false
floatsintext      : true

documentclass     : "apa6"
classoption       : "man"
numbersections    : false
link-citations    : true

output:
    # papaja::apa6_word:
    #     toc: no
    papaja::apa6_pdf:
        toc: no
        toc_depth: 3
        highlight: default
        latex_engine: xelatex
        
date: "`r format(Sys.time(), '%d-%m-%Y')`"

header-includes   :
- \usepackage{booktabs}
- \usepackage{amsmath}
- \usepackage[american]{babel}
- \usepackage[utf8]{inputenc}
# - \usepackage[T1]{fontenc}
- \usepackage{sectsty} \allsectionsfont{\raggedright} # left-align H1 titles
bibliography: ["`r rbbt::bbt_write_bib('references/references.bib', overwrite = TRUE)`", "references/r-references.bib"]
---

```{r setup, include=FALSE}
## load libraries
library(knitr)
library(tidyverse)
library(lme4)
library(afex)
library(emmeans)
library(papaja)

options(emmeans=list(msg.interaction=FALSE))
theme_set(theme_apa())

# set global chunk options, put figures into folder
options(tinytex.verbose = TRUE)
options(warn=-1, replace.assign=TRUE)
knitr::opts_chunk$set(
  echo = FALSE,
  include = TRUE,
  # warning = FALSE,
  fig.align = "center",
  fig.path = "figures/figure-",
  fig.show = "hold",
  fig.width=7, fig.asp =0.618,
  width = 1800, 
  message = FALSE
)

source(here::here("R", "funcs_sim.R"))

set.seed(2022)
```

\appendix

# (Pre-)specifying effects of interest in practice

## Introduction to the hypothetical study

The aim of this hypothetical training study was to test whether the training protocol is effective in improving participants' ability of recognizing other-race faces. Please refer to the main text for more details. The specific experimental design is as followings:

Independent variables:
- 2 (Group: `train` vs. `control`; between-subject factor)
- 2 (Test: `pre-` vs. `post-test`; within-subject factor)

Dependent variables:
- Behavioral choices (analyzed with sensitivity _d_'; `d`)
- (correct) Response times (analyzed with log-normal transformation; `rt`)

## Data simulation

Let us simulate two data sets for behavioral choices and response times separately with the above design of 30 participants for each group. For simplicity, the average performance for each participant was simulated directly. Please refer to the codes in the open materials for detailed simulation procedures. Table \@ref(tab:simu-true) displays the ground truth for each condition, and the structure of the simulated data is as follows:

```{r}
d_pre <- 2.5
d_delta <- c(0.4, 0.9) # control, train
d_post <- d_pre + d_delta
df_d <- sim_train(d_pre, d_delta, std=0.5, dvname="d")
```

```{r}
str(df_d)
```

```{r simulating rt data}
rt_pre <- 6.4
rt_delta <- c(-0.3, -0.3)
rt_post <- rt_pre + rt_delta 
df_rt <- sim_train(rt_pre, rt_delta, std=0.1, dvname="logrt", logrt=TRUE)
```

```{r}
str(df_rt)
```

```{r simu-true}
tibble(Test = c("pre-test", "post-test"),
       d_control = c(d_pre, d_delta[1]),
       d_train = c(d_pre, d_delta[2]),
       rt_control = c(round(exp(rt_pre)), round(exp(rt_delta[1]))),
       rt_train = c(round(exp(rt_pre)), round(exp(rt_delta[2])))) %>% 
  apa_table(caption='The ground truth for each condition.',
            note='Population parameters used to simulate data for the hypothetical training study.')
```

## Specifying EOI

As discussed in the main text, the EOI for concluding that the training is effective is: ($d^{'}_{post\_train} > d^{'}_{pre\_train}$ and $d^{'}_{post\_train-pre\_train} > d^{'}_{post\_control-pre\_control}$) or ($rt_{post\_train} < rt_{pre\_train}$ and $rt_{post\_train-pre\_train} < rt_{post\_control-pre\_control}$). 


# Conclusion-based Type I error rate in the hypothetical training study

The Conclusion-based Type I Error Rate (CBER) in the hypothetical training study refereed to the probability of concluding the training effectiveness with the specified EOI when the statistical null hypotheses for all the tests included in EOI were true. CBER was calculated with following simulation.


```{r}
# simulate null data set and calculate the p-values for single test
df_sim_null <- sim_train_null(iter=10000, N_subj=30, N_core=16, 
                     file_cache = here::here("simulation", "sim_train_null.rds"))
```

```{r}
# apply alpha (0.05) to the simulated null data
df_typeI_train <- sim_train_cber(df_sim_null, 0.05) %>% 
  summarize(Type_I_d = mean(sig_d_both), 
            Type_I_rt = mean(sig_rt_both), 
            Type_I_or = mean(sig_d_or_rt))
```

In each iteration (10,000 in total), one null data set for _d_' and one for RT were simulated separately for 30 participants. Then a mixed-ANOVA, whose procedure was identical to the steps in Appendix A, was performed on _d_' and RT separately, where the same simple effect and the two-way interaction were inspected. The simulation codes (`sim_train_null`) are available in the open materials. The simulation results showed that the CBER for concluding training effectiveness is `r df_typeI_train$Type_I_or`.

The same procedure was repeated for varied sample sizes (30, 50, 75, 100, 150, 200, or 300) with 5,000 iterations for each. Results (\@ref(fig:simu-cber) and (Table \@ref(tab:sim-cber-Ns))) revealed similar patterns for different sample sizes (as indicated by the slightly transparent colors). First, the Type I error rate for single test, including the simple effect and the two-way interaction for both _d_' and RT, matched the applied alpha, as shown by the ??? lines. For example, with the alpha of 5%, the Type I error rate of single test is also about 5%. Second, the Conjunction Type I error rate, in which both the simple effect and interaction were significant conjunctly when their corresponding statistical null hypotheses were true, for _d_' or RT is smaller than half of the alpha, as shown by the ??? lines. For instance, the Conjunction Type I error rate is about 1.8% with the conventional alpha of 5% for single tests. Last and most importantly, the CBER for concluding training effectiveness is smaller than the applied alpha, as shown by the black solid lines. When the alpha of 6.6% (as indicated by the vertical dash line) was applied to single tests, the CBER for claiming the training effectiveness is about 5%. 

```{r}
# simulate null data with varied sample sizes
df_sim_null_Ns <- sim_train_null(
  iter=5000, 
  N_subj=c(30, 50, 75, 100, 150, 200, 300), 
  N_core=16, 
  file_cache = here::here("simulation", "sim_train_null_Ns.rds"))
```

```{r sim-cber-Ns}
sim_train_cber(df_sim_null_Ns, 0.05) %>% 
  group_by(N_subj) %>%
  summarize(N_subj = printnum(N_subj, digits=0),
            d_simple = printnum(mean(sig_d_simple), digits=3),
            d_interaction = printnum(mean(sig_d_inter), digits=3),
            rt_simple = printnum(mean(sig_rt_simple), digits=3),
            rt_interaction = printnum(mean(sig_rt_inter), digits=3),
            d_both = printnum(mean(sig_d_both), digits=3), 
            rt_both = printnum(mean(sig_rt_both), digits=3), 
            CBER = printnum(mean(sig_d_or_rt), digits=3)) %>% 
  apa_table(caption='Conclusion-based and other Type I error rates',
            note='Type I error rates at different levels for varied sample sizes.')
```

(ref:simu-cber-caption) Conclusion-based and other Type I error rates with varied $\alpha$ for single tests in the hypothetical training study. Similar results were obtained for varying sample sizes (as indicated by slightly transparent lines). (1) The dotted lines display that the Type I error rates for single tests match the corresponding alpha. (2) The dashed lines shows that the Conjunction Type I error rate (CER) for _d_' or RT was smaller than half of the alpha for single tests. (3) The solid lines display the CBER. Specifically, the CBER is about 5% when the alpha of 6.6% is applied to single tests. 

Single Test Type I Error Rate (STER) for the interaction and simple effect, respectively (the two types of dashed lines overlap with each other). The solid line denotes the Hypothesis-based Type I Error Rate (HER) for claiming the composite face effect, i.e., the Conjunction Type I Error Rate where the interaction and simple effect are significant at the same time. The gray dashed and solid lines denote STER and HER for simulated data with different sample sizes (varying from 10 to 1000), and black lines are the average error rates across sample sizes. STER and HER are similar for data with different sample sizes. STER for the interaction and simple effect match the corresponding $\alpha$, while HER is about half of the $\alpha$. For instance, when the $\alpha$ of 0.1 is applied to single tests indicated by the vertical dashed line, HER is just below 0.05, the conventional threshold indicated by the horizontal dashed line.

```{r simu-her, fig.asp=.75, fig.cap="(ref:simu-cber-caption)"}
alpha_ls <- c(.3, .2, .15, .1, .05, .01, .001, .0001)
df_alpha <- map_dfr(alpha_ls, sim_train_cber,
               df_sim=df_sim_null_Ns, disp=F, .id="alpha_int") %>%
  mutate(alpha=alpha_ls[as.integer(alpha_int)],
         dv=case_when(
           grepl("_or_", effects, fixed = T) ~ "CBER",
           grepl("_d_", effects, fixed = T) ~ "d",
           grepl("_rt_", effects, fixed=T) ~ "RT",
           TRUE ~ "NA"
         ),
         level=case_when(
           grepl("_or_", effects, fixed = T) ~ "CBER",
           grepl("_both", effects, fixed = T) ~ "both",
           grepl("_simple", effects, fixed = T) ~ "simple",
           grepl("_inter", effects, fixed = T) ~ "interaction",
           TRUE ~ "NA"
         ),
         dv = factor(dv, levels=c("CBER", "d", "RT")),
         level = factor(level, levels=c("CBER", "both", "simple", "interaction")))

df_alpha_mean <- df_alpha %>%
  group_by(level, dv, alpha) %>%
  select(level, dv, alpha, Type_I) %>%
  summarize(Type_I_mean = mean(Type_I), .groups = "drop")

df_alpha %>%
  ggplot(aes(alpha, Type_I, linetype=level, shape=level, color=dv)) +
  geom_point(alpha=.5, show.legend=F) +
  geom_line(aes(group=interaction(N_subj, level, dv, sep = "-")), alpha=.2, show.legend=F) +
  geom_point(data=df_alpha_mean, aes(y=Type_I_mean), size=3, show.legend=F) +
  geom_line(data=df_alpha_mean, aes(y=Type_I_mean), size=1) +
  geom_hline(yintercept = .05, linetype="dashed") +
  geom_vline(xintercept = .066, linetype="dashed") +
  scale_color_manual(values = c("black", "#E69F00", "#56B4E9")) +
  scale_linetype_manual(labels=c("CBER", "CER (interaction+simple)", "interaction", "the simple effect"),
                          values=c("solid", "dashed", "dotted", "dotted")) +
  scale_y_continuous(breaks=seq(0,.3,.05)) +
  scale_x_continuous(breaks=seq(0,.3,.05)) +
  labs(x=expression(alpha~" for a single test"),
       y="Type I error rate") +
  theme(legend.title = element_blank(),
        legend.position = c(0.32, 0.87),
        legend.box = "horizontal") +
  NULL

```

```{r}
r_refs(file = "references/r-references.bib")
my_citations <- cite_r(file = "references/r-references.bib")
```


\newpage

**References**

