---
title             : 'Appendices for "The necessity of pre-specifying effects of interest in identifying robust phenomena for theory construction"'
shorttitle         : "Pre-specifiying effects of interest"

author: 
  - name: "Haiyang Jin"
    affiliation: ""
    address: ""
    email: "haiyang.jin@outlook.com"
    corresponding: yes
    
affiliation:
  - id            : ""
    institution   : ""
  
authornote        : |
  This is the appendices for "The necessity of pre-specifying effects of interest in identifying robust phenomena for theory construction".
keywords          : "effects of interest; conclusion credibility; robust phenomena"
wordcount         : "X"

figurelist        : false
tablelist         : false
footnotelist      : false
linenumbers       : false
mask              : false
draft             : false
floatsintext      : true

documentclass     : "apa6"
classoption       : "man"
numbersections    : false
link-citations    : true

output:
    # papaja::apa6_word:
    #     toc: no
    papaja::apa6_pdf:
        toc: no
        toc_depth: 3
        highlight: default
        latex_engine: xelatex
        
date: "`r format(Sys.time(), '%d-%m-%Y')`"

header-includes   :
- \usepackage{booktabs}
- \usepackage{amsmath}
- \usepackage[american]{babel}
- \usepackage[utf8]{inputenc}
# - \usepackage[T1]{fontenc}
- \usepackage{sectsty} \allsectionsfont{\raggedright} # left-align H1 titles
bibliography: ["`r rbbt::bbt_write_bib('references/references.bib', overwrite = TRUE)`", "references/r-references.bib"]
---

```{r setup, include=FALSE}
## load libraries
library(knitr)
library(tidyverse)
library(lme4)
library(afex)
library(emmeans)
library(papaja)

options(emmeans=list(msg.interaction=FALSE))
theme_set(theme_apa())

# set global chunk options, put figures into folder
options(tinytex.verbose = TRUE)
options(warn=-1, replace.assign=TRUE)
knitr::opts_chunk$set(
  echo = FALSE,
  include = TRUE,
  # warning = FALSE,
  fig.align = "center",
  fig.path = "figures/figure-",
  fig.show = "hold",
  fig.width=7, fig.asp =0.618,
  width = 1800, 
  message = FALSE
)

source(here::here("R", "funcs_sim.R"))

set.seed(2022)
```

```{r}
r_refs(file = "references/r-references.bib")
my_citations <- cite_r(file = "references/r-references.bib")
```

\appendix

# Pre-specifying effects of interest in practice

## A hypothetical study

The hypothetical training study aimed to test whether the training protocol was effective in improving participants' ability of recognizing other-race faces. A 2 (Group: `train` vs. `control`; between-subject factor) $\times$ 2 (Test: `pre-test` vs. `post-test`; within-subject factor) mixed design was employed. The dependent variables included both the behavioral choices and correct response times. Specifically, behavioral choices were analyzed with sensitivity _d_' in signal detection theory, and correct response times (RT) were analyzed with log-normal transformation. Please refer to the main text for more details.

## Data simulation

To demonstrate how to pre-specify effects of interest (EOI) in practice, data sets were simulated for behavioral choices and response times separately. There were 30 participants in each group. For simplicity, the performance of each condition for each participant was simulated directly. Please refer to the open materials (https://osf.io/yhm3s/) for detailed simulation procedures. Table \@ref(tab:simu-true) displays the ground truth for each condition. The structure of the simulated data _d_' is:
```{r}
d_pre <- 2.5
d_delta <- c(0.4, 0.9) # control, train
d_post <- d_pre + d_delta
df_d <- sim_train(d_pre, d_delta, std=0.5, dvname="d")
```

```{r}
str(df_d)
```

```{r simulating rt data}
logrt_pre <- 6.4
logrt_delta <- c(-0.1, -0.1)
logrt_post <- logrt_pre + logrt_delta 
df_rt <- sim_train(logrt_pre, logrt_delta, std=0.1, dvname="logrt", logrt=TRUE)
```

The structure of the simulated data RT is:
```{r}
str(df_rt)
```

```{r simu-true}
tibble(Test = c("pre-test", "post-test"),
       d_control = c(d_pre, d_pre+d_delta[1]),
       d_train = c(d_pre, d_pre+d_delta[2]),
       rt_control = c(printnum(exp(logrt_pre), digits=0),
                      printnum(exp(logrt_pre+logrt_delta[1]), digits=0)),
       rt_train = c(printnum(exp(logrt_pre), digits=0),
                    printnum(exp(logrt_pre+logrt_delta[2]), digits=0))) %>% 
  apa_table(caption='The ground truth for each condition.',
            note='Population parameters used to simulate data for the hypothetical training study.')
```

## Specifying EOI

As discussed in the main text, the EOI for concluding that the training was effective was: ($d^{'}_{post\_train} > d^{'}_{pre\_train}$ and $d^{'}_{post\_train-pre\_train} > d^{'}_{post\_control-pre\_control}$) or ($rt_{post\_train} < rt_{pre\_train}$ and $rt_{post\_train-pre\_train} < rt_{post\_control-pre\_control}$). 

## Data analysis

The $\alpha$ of 5% would be used at the single test level for all the single tests and, therefore, the Conclusion-based Type I Error Rate (CBER) would not be higher than 5% (see Appendix C). Alternatively, one may use $\alpha$ of 6.6% for single tests and the corresponding CBER is about 5% (see Appendix C).

### Sensitivity d'

A 2 $\times$ 2 mixed-ANOVA was first performed on sensitivity _d_':

```{r echo=TRUE}
d_aov <- aov_4(d ~ Group * Test + (Test | Subj), df_d)
```

The results were not displayed as none of these effects could test the directional effects specified in EOI. Thus, we first tested that simple effect (i.e., post-test vs. pre-test in the training group) with the one-tailed test:

```{r echo=TRUE}
d_emm <- emmeans(d_aov, ~ Group + Test)
summary(contrast(d_emm, "revpairwise", by="Group")[2], side=">", infer=T)
```

The above result showed that the _d_' in the post-test was significantly better than the pre-test in the training group. Next, we tested the two-way interaction (i.e., the increase of _d_' in the training group relative to the control group) with the one-tailed test:

```{r echo=TRUE}
summary(contrast(d_emm, interaction="revpairwise"), side=">", infer=T)
```
This result showed that the increase of _d_' from pre- to post-test was significantly greater in the training group relative to the control group. 

### Correct response times

A similar 2 $\times$ 2 mixed-ANOVA was performed on RT:

```{r echo=TRUE}
logrt_aov <- aov_4(logrt ~ Group * Test + (Test | Subj), df_rt)
```

Next, we tested that simple effect (i.e., post-test vs. pre-test in the training group) with the one-tailed test:

```{r echo=TRUE}
logrt_emm <- emmeans(logrt_aov, ~ Group + Test)
summary(contrast(logrt_emm, "revpairwise", by="Group")[2], side="<", infer=T)
```

This result showed that RT in post-test was shorter than the pre-test in the training group. Next, we examined the two-way interaction (i.e., the decrease of RT in the training group relative to the control group) with the one-tailed test:

```{r echo=TRUE}
summary(contrast(logrt_emm, interaction="revpairwise"), side="<", infer=T)
```
The magnitude of the RT decrease was not significantly greater in the training relative to the control group. Therefore, the RT results did not provide evidence that the training was effective. Notably, this result did not suggest that the training protocol was not effective, neither.

In summary, with the results of both _d_' and RT as specified in EOI, we may claim that the training protocol was effective.

# Re-analyze the simulated data with multilevel modeling

Specifying EOI is not specific to any statistical method. Appendix A demonstrated how to employ ANOVA to inspect EOI. In this appendix, multilevel modeling (also known as linear mixed-effects modeling or hierarchical modeling) is used to inspect the same EOI with the same simulated data sets. Following the analysis in Appendix A, behavioral choices were also analyzed with signal detection theory (sensitivity _d_') and RT was analyzed with log-normal transformation. Of note, it is typically more beneficial to perform multilevel modeling on the trial-level data and incorporate the full random-effect structure. For simplicity, multilevel modeling employed in this appendix will only incorporate the by-subject random intercept and be performed on the condition means for each participant. Anyhow, this simplification would not invalidate the following demonstration. Please refer to the open materials of @jinTwoFacesHolistic for more appropriate applications of multilevel modeling with signal detection theory and log-transformation.

## Sensitivity d'

The same EOI would be inspected here: (1) _d_' in the post-test relative to the pre-test in the training group, and (2) the increase of _d_' in the training group relative to the control group. It happened that these two effects could be tested with the default dummy contrast coding when the default level in each factor was set appropriately.

```{r, echo=T}
# set the level order for each factor
df_d_lmm <- df_d %>% 
  mutate(Test = factor(Test, levels=c("post", "pre")), 
         Group = factor(Group, levels=c("train", "control")))
```

The hierarchical model for _d_' was:
```{r, echo=T}
d_lmm <- lmer(d ~ Group * Test + (1 | Subj), data = df_d_lmm)
round(summary(d_lmm)$coefficients, 3)
```

These results suggest that in the training group, the _d_' in the pre-test was worse than that in the post-test (results of `Testpre`) and the increase of _d_' in the training group was greater than the control group (results of `Groupcontrol:Testpre`). Noteworthy that these results (mainly the p-values) were from two-tailed tests. Since the effects specified in EOI were directional, we might use one-tailed instead of two-tailed tests. For example, we might use a t-value of 1.65 instead of 1.96 as the threshold to claim statistical significance when $\alpha$ of 5% is employed. Alternatively, `library(emmeans)` could be used to perform the one-tailed test, similar to Appendix A. 

```{r echo=TRUE}
d_lmm_emm <- emmeans(d_lmm, ~ Group + Test)
summary(contrast(d_lmm_emm, "pairwise", by="Group")[1], side=">", infer=T)
```

The above result showed that the _d_' in the post-test was significantly better than the pre-test in the training group. Next, we tested the two-way interaction (i.e., the increase of _d_' in the training group relative to the control group) with the one-tailed test:

```{r echo=TRUE}
summary(contrast(d_lmm_emm, interaction="pairwise"), side=">", infer=T)
```

This result showed that the increase of _d_' in the training group was significantly greater than that in the control group.

## Correct Response times

```{r}
# set the levels in each factor
df_rt_lmm <- df_rt %>% 
  mutate(Test = factor(Test, levels=c("post", "pre")), # pre is the first level 
         Group = factor(Group, levels=c("train", "control"))) # control is the first level
```

The similar multilevel model for RT is:

```{r, echo=T}
logrt_lmm <- lmer(logrt ~ Group * Test + (1 | Subj), data = df_rt_lmm)
round(summary(logrt_lmm)$coefficients, 3)
```

Note that the above results were from two-tailed tests. Therefore, we used `library(emmeans)` to perform the one-tailed test. 

```{r echo=TRUE}
logrt_lmm_emm <- emmeans(logrt_lmm, ~ Group + Test)
summary(contrast(logrt_lmm_emm, "pairwise", by="Group")[1], side="<", infer=T)
```

This result showed that the _d_' of the post-test was significantly better than the pre-test in the training group. Next, we tested the two-way interaction with the one-tailed test:

```{r echo=TRUE}
summary(contrast(logrt_lmm_emm, interaction="pairwise"), side="<", infer=T)
```

We failed to observe the evidence that the decrease of RT in the training group was larger than the control group. With results of both _d_' and RT, we may claim that the training was effective. 

# Conclusion-based Type I error rate in the hypothetical training study

The CBER in the hypothetical training study refereed to the probability of concluding the training effectiveness when the statistical null hypotheses for all the tests specified in EOI were true. CBER in the hypothetical training study was estimated with following simulation.

```{r}
# simulate null data set and calculate the p-values for single test
df_sim_null <- sim_train_null(iter=10000, N_subj=30, N_core=16, 
                     file_cache = here::here("simulation", "sim_train_null.rds"))
```

```{r}
# apply alpha (0.05) to the simulated null data
df_typeI_train <- sim_train_cber(df_sim_null, 0.05) %>% 
  summarize(Type_I_d = mean(sig_d_both), 
            Type_I_rt = mean(sig_rt_both), 
            Type_I_or = mean(sig_d_or_rt))
```

In each of 10,000 iterations, one null data set for _d_' and one for RT were simulated separately for 30 participants. Then a mixed-ANOVA, in which the procedure was identical to the steps in Appendix A, was performed on _d_' and RT separately, where the same simple effect and the two-way interaction were inspected. The simulation codes are available in the open materials (https://osf.io/yhm3s/). The simulation results showed that the CBER for concluding training effectiveness is `r df_typeI_train$Type_I_or*100`% when the $\alpha$ of 5% was applied at the single test level.

Next, simulation was employed to explore the relationships between the $\alpha$ applied at the the single test level and the corresponding Type I error rates at the single test and dependent variable levels, as well as at the conclusion level. Specifically, the same procedure was repeated for varied sample sizes (30, 50, 75, 100, 150, 200, or 300) with 5,000 iterations for each. Results (Figure \@ref(fig:simu-cber) and Table \@ref(tab:sim-cber-Ns)) revealed similar patterns for different sample sizes (as indicated by the slightly transparent lines). First, the Type I error rates for single tests, including the simple effect and the two-way interaction for both _d_' and RT, matched the applied $\alpha$, as shown by yellow (_d_') and blue (RT) dotted lines. For example, with the $\alpha$ of 5%, the Type I error rate of single test is also about 5%. Second, the Conjunction Type I error rate, in which both the simple effect and interaction were significant at the same time when their corresponding statistical null hypotheses were true, for _d_' or RT is smaller than half of the $\alpha$, as shown by the yellow (_d_') and blue (RT) dashed lines. For instance, the Conjunction Type I error rate is about 1.8% with the conventional $\alpha$ of 5% for single tests. Last and most importantly, the CBER for concluding training effectiveness is smaller than the applied $\alpha$, as shown by the black solid lines. When the $\alpha$ of 6.6% (as indicated by the vertical dashed line) is applied to single tests, the CBER for claiming the training effectiveness is about 5%. 

```{r}
# simulate null data with varied sample sizes
df_sim_null_Ns <- sim_train_null(
  iter=5000, 
  N_subj=c(30, 50, 75, 100, 150, 200, 300), 
  N_core=16, 
  file_cache = here::here("simulation", "sim_train_null_Ns.rds"))
```

```{r sim-cber-Ns}
sim_train_cber(df_sim_null_Ns, 0.05) %>% 
  group_by(N_subj) %>%
  summarize(N = printnum(N_subj, digits=0),
            d_simple = printnum(mean(sig_d_simple), digits=3),
            d_interaction = printnum(mean(sig_d_inter), digits=3),
            rt_simple = printnum(mean(sig_rt_simple), digits=3),
            rt_interaction = printnum(mean(sig_rt_inter), digits=3),
            d_both = printnum(mean(sig_d_both), digits=3), 
            rt_both = printnum(mean(sig_rt_both), digits=3), 
            CBER = printnum(mean(sig_d_or_rt), digits=3)) %>% 
  select(-N_subj) %>% 
  apa_table(caption='Conclusion-based and other Type I error rates with the alpha of 0.05.',
            note='Type I error rates at different levels for simulation with varied sample sizes. N denotes the number of participants. Columns starting with "d" or "RT" display the results for sensitivity d and response times separately. Columns ending with "simple" denote the comparisons between pre- and post-test in the training group. Columns ending with "interaction" denote the comparison of performance changes (i.e., post-test relative to pre-test) between the training and control groups. Columns ending with "both" denote the Type I error rate when both the simple effect and interaction were significant at the same time for that dependent variable. CBER refers to the conclusion-based Type I error rate, i.e., the EOI.')
```

(ref:simu-cber-caption) Conclusion-based and other Type I error rates with varied $\alpha$ for single tests in the hypothetical training study. Similar results were obtained for different sample sizes (as indicated by slightly transparent lines). (1) The dotted lines display that the Type I error rates for single tests match the corresponding $\alpha$ (_d_': yellow; RT: blue). (2) The dashed lines show that the Conjunction Type I error rate (CER) for _d_' (yellow) or RT (blue) is smaller than half of the $\alpha$ for single tests. (3) The solid lines display the relationships between the CBER and $\alpha$ for single tests. Specifically, the CBER is about 5% when $\alpha$ of 6.6% is applied to single tests, as indicated by the vertical dashed line. 

```{r simu-cber, fig.asp=.75, fig.cap="(ref:simu-cber-caption)"}
alpha_ls <- c(.3, .2, .15, .1, .05, .01, .001, .0001)
df_alpha <- map_dfr(alpha_ls, sim_train_cber,
               df_sim=df_sim_null_Ns, disp=F, .id="alpha_int") %>%
  mutate(alpha=alpha_ls[as.integer(alpha_int)],
         dv=case_when(
           grepl("_or_", effects, fixed = T) ~ "CBER",
           grepl("_d_", effects, fixed = T) ~ "d",
           grepl("_rt_", effects, fixed=T) ~ "RT",
           TRUE ~ "NA"
         ),
         level=case_when(
           grepl("_or_", effects, fixed = T) ~ "CBER",
           grepl("_both", effects, fixed = T) ~ "both",
           grepl("_simple", effects, fixed = T) ~ "simple",
           grepl("_inter", effects, fixed = T) ~ "interaction",
           TRUE ~ "NA"
         ),
         dv = factor(dv, levels=c("CBER", "d", "RT")),
         level = factor(level, levels=c("CBER", "both", "simple", "interaction")))

df_alpha_mean <- df_alpha %>%
  group_by(level, dv, alpha) %>%
  select(level, dv, alpha, Type_I) %>%
  summarize(Type_I_mean = mean(Type_I), .groups = "drop")

df_alpha %>%
  ggplot(aes(alpha, Type_I, linetype=level, shape=level, color=dv)) +
  geom_point(alpha=.5, show.legend=F) +
  geom_line(aes(group=interaction(N_subj, level, dv, sep = "-")), alpha=.2, show.legend=F) +
  geom_point(data=df_alpha_mean, aes(y=Type_I_mean), size=3, show.legend=F) +
  geom_line(data=df_alpha_mean, aes(y=Type_I_mean), size=1) +
  geom_hline(yintercept = .05, linetype="dashed") +
  geom_vline(xintercept = .066, linetype="dashed") +
  scale_color_manual(values = c("black", "#E69F00", "#56B4E9")) +
  scale_linetype_manual(labels=c("CBER", "CER (interaction+simple)", "interaction", "the simple effect"),
                          values=c("solid", "dashed", "dotted", "dotted")) +
  scale_y_continuous(breaks=seq(0,.3,.05)) +
  scale_x_continuous(breaks=seq(0,.3,.05)) +
  labs(x=expression(alpha~" for a single test"),
       y="Type I error rate") +
  theme(legend.title = element_blank(),
        legend.position = c(0.32, 0.87),
        legend.box = "horizontal") +
  NULL

```

# Software information
These appendices were created with `r my_citations`.

\newpage

**References**

